{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQA on CLEVR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the vocabulary\n",
    "First of all I created the vocabulary using the `Tokenizer` method. I loaded the train dataset and fitted the Tokenizer to create the unique vocabulary.\n",
    "The vocabulary has 71 words and it is used for the creation of the datasets.\n",
    "\n",
    "## Creation of the datasets\n",
    "\n",
    "The dataset has more than 250k questions at which we have to associate the corresponding images and answers. I put a lot of effort in trying to optimize the input pipeline for the training otherwise, even running on GPU, the training process wpuld have taken lots of time. \n",
    "\n",
    "At first I tried using generators to fetch data. Using the following method \n",
    "\n",
    "    while True: \n",
    "        for i in range(batch_size): \n",
    "            ... load data here ...\n",
    "            ... preprocess data here ...\n",
    "            yield [question, image], answer`\n",
    "\n",
    "but I've found that this method lack of parallelizing power which is what we want for optimizing the fetching of the data. \n",
    "I did another attempt using the methods of the `tensorflow` input pipeline library `tf.Data`. In this way I was able to create dataset that were both able to load and process batches of data in parallel and threadable for the `mode.fit()` procedure. \n",
    "\n",
    "The pipeline follows these steps (for train/validation datasets): \n",
    "* load the whole list of data from the json file (as a disctionary)\n",
    "* shuffle all the entries of the dictionary (but, obviously, keeping the triple {question, answer, img_filepath} together)\n",
    "* split train and validation data\n",
    "* preprocess _all_ the data:\n",
    "    - answers were encoded in one hot vectors;\n",
    "    - questions were tokenized; \n",
    "    - the images_filepath was converted in a full path (from the root) and will be loaded in a few steps; \n",
    "* create the datasets using `tf.data.dataset.from_tensor_slices`. \n",
    "----\n",
    "A note, even though the questions are a lot Python manage quite well the preprocessing on text, finishing it in a couple of second. \n",
    "I decided, then, to do all the text processing in advace in order not delay the loading of the images (surely the bottleneck of the pipeline).\n",
    "\n",
    "---\n",
    "Once the dataset are created I performed the following operations: \n",
    "* additional shuffling (not _strictly_ necessary since the dataset has already been shuffled before train/valid splitting)\n",
    "* using the `.repeat()` method. This allows to cicle through the dataset during the various epochs\n",
    "* mapped the dataset to a `parse_function` for loading the data in parallel. `dataset.map` allows to use multiple parallel calls to map the dataset in the target function, this allowed an optimized pipeline. The `parse_function` loaded the images in the following way: \n",
    "    - load the images using `tf.io.read_file(img_filenames)`\n",
    "    - decode the png format using `tf.image.decode_png(img, channels=3)`\n",
    "    - convert the images to `float32` type and perform proper resizing\n",
    "* divide in batches\n",
    "* prefetch some batches in memory according to the system performances\n",
    "\n",
    "Using the `tf.data` dataset I managed to improve the efficiency of the training stage by nearly 50%. With shallow models a single epoch take less than half an hour while for more complex systems, with transfer learning and fine tuning a single epoch took a full hour to train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import vqa_utils as vqa\n",
    "\n",
    "# set the seed and check for GPU using the proper functions\n",
    "vqa.set_seed(1234)\n",
    "vqa.check_gpu()\n",
    "\n",
    "batch_size = 8\n",
    "# define fitting parameters, max_queue_size, steps_per_epoch, validation_steps according to the batch size.\n",
    "max_que,e_steps,v_steps = vqa.fit_param(batch_size)\n",
    "\n",
    "sentence_tokenizer = vqa.create_tokens()\n",
    "\n",
    "train_dataset = vqa.get_dataset(sentence_tokenizer, batch_size)\n",
    "valid_dataset = vqa.get_dataset(sentence_tokenizer, batch_size, is_training=False)\n",
    "test_dataset = vqa.test_dataset(sentence_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation \n",
    "\n",
    "I reported here my best model for this task. Since the train dataset is big enough I decided use transfer learning only for the feature extraction from the images, the Embedding Layer is trained from scratch. \n",
    "\n",
    "The basic structure of the network is the same for all these models. It is Y-shaped network with two inputs and a single output. One input takes the tokenized (and padded) questions and the other the images. After the embedding/LSTM and feature extraction the two branches are jointed together and classified with a fully connected (FC) network.\n",
    "\n",
    "For the extraction of the image features I opted for transfer learning with fine tuning. I experimented with\n",
    "- VGG16\n",
    "- ResNet50\n",
    "- DenseNet\n",
    "\n",
    "pre-trained on the ImageNet dataset. \n",
    "\n",
    "Even though vanilla ResNet and DenseNet gave better results with respect to VGG16, they're also deeper and more complex networks meaning that a fine-tuning procedure was hardly feasible. I kept the model with VGG16 because it gave me the flexibility of having a big network for extracting meaningful features but also the possibility of fine tune the weights on the particular dataset (CLEVR).\n",
    "\n",
    "The final model is constructed in the following way: \n",
    "- Embedding and LSTM for the question; \n",
    "- VGG16 with a 5 layer fine tuning for feature extraction and a final `GlobalAveragingPooling` layer; \n",
    "\n",
    "The Global Averaging Pooling was used to extract an array of features from the output of the CNN. It was choosen on the `Flatten()` alternative because of its regularization property and the fact that it has reduced the overall number of parameters.\n",
    "\n",
    "The two representations are then mapped to (512,) array using `Dense` layers with `linear` activation function and multiplied pointwise for merging them. I also implemented a similar model using concatenation as a merging technique but I did find that the pointwise multiplication worked better. \n",
    "\n",
    "At this point I built 3 FC classfier in parallel each one of them with a different number of activation and initialization of the layer. This allow me to mimic an ensemble method but using a single, end-to-end trained network. This is the best trade off considering the limits of GPU availability in the Kaggle platform.\n",
    "\n",
    "The results of the classifiers are averaged into a single layer. \n",
    "\n",
    "---\n",
    "As loss function I used a focal loss in order to perform classification since it penalizes more the misclassified points with respect to the correctly classified ones. This allows to take care of the big class unbalance in the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 71\n",
    "SENTENCE_LENGTH = 42\n",
    "NUM_ANS = 13\n",
    "IMG_WIDTH = 480\n",
    "IMG_HEIGHT = 320\n",
    "\n",
    "def get_model(): \n",
    "    \n",
    "    def clf_net(activation, SEED):\n",
    "        \"\"\"\n",
    "        function the builds the classification network. \n",
    "        input param: \n",
    "        activation: number of activation in the Dense layre\n",
    "        SEED: seed for the initialization of the layer initializer\n",
    "        \"\"\"\n",
    "        init_x = tensorflow.keras.initializers.glorot_normal(seed=SEED)\n",
    "        \n",
    "        x = tensorflow.keras.layers.Dense(activation,activation='relu', kernel_initializer=init_x)(merge_layer)\n",
    "        x = tensorflow.keras.layers.Dropout(0.5)(x)\n",
    "        answer = tensorflow.keras.layers.Dense(NUM_ANS, activation='softmax')(x)\n",
    "        return answer\n",
    "\n",
    "    # QUESTION BRANCH    \n",
    "    question_input = tensorflow.keras.layers.Input(shape=(SENTENCE_LENGTH,), name='input_qst')\n",
    "    x = tensorflow.keras.layers.Embedding(MAX_NUM_WORDS+1, output_dim=50, mask_zero=True)(question_input)\n",
    "    x = tensorflow.keras.layers.LSTM(units=256)(x)\n",
    "    x = tensorflow.keras.layers.Dropout(0.3)(x)\n",
    "    question_output = tensorflow.keras.layers.Dense(units=512, activation='linear')(x)\n",
    "    \n",
    "    # IMAGES BRANCH \n",
    "    base_model = tensorflow.keras.applications.VGG16(include_top=False, weights='imagenet')\n",
    "    \n",
    "    # fine tune the last 5 layers\n",
    "    for layer in base_model.layers[:-5]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    images_input = tensorflow.keras.layers.Input(shape=(IMG_WIDTH,IMG_HEIGHT,3), name='input_img')\n",
    "    x = base_model(images_input)\n",
    "    x = tensorflow.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    images_output = tensorflow.keras.layers.Dense(units=512, activation='linear')(x)\n",
    "\n",
    "    # MERGING \n",
    "    merge_layer = tensorflow.keras.layers.multiply([question_output,images_output])\n",
    "    \n",
    "    # FC CLASSIFIER\n",
    "    answer_x = clf_net(512, SEED=13)\n",
    "    answer_y = clf_net(256, SEED=37)\n",
    "    answer_z = clf_net(128, SEED=89)\n",
    "    \n",
    "    # OUTPUT\n",
    "    output_ = tensorflow.keras.layers.average([answer_x, answer_y, answer_z])\n",
    "\n",
    "    model = tensorflow.keras.models.Model([question_input, images_input], output_)\n",
    "\n",
    "    loss = vqa.focal_loss(alpha=1.)\n",
    "    optimizer = tensorflow.keras.optimizers.Adam(learning_rate=5e-4)\n",
    "    metrics = [\n",
    "      tensorflow.keras.metrics.CategoricalAccuracy(name='acc'),\n",
    "      tensorflow.keras.metrics.Precision(name='prec'),\n",
    "      tensorflow.keras.metrics.Recall(name='rec'),\n",
    "      tensorflow.keras.metrics.AUC(name='auc'),\n",
    "    ]\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "tensorflow.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the hyperparameters has been choosen by baby sitting the model and run it various times in order to improve the performances. In particular I did focus on: \n",
    "- the output dimension of the embedding layer;\n",
    "- the number of units of the LSTM (I tried with 8,18,32,...,512 and find the optimum in the performances/epoch-time trade of at 256);\n",
    "- the dimesions of the array for the merging layer;\n",
    "- the learning rate (Initially 1e-3 and scaled down to 5e-4).\n",
    "\n",
    "All of these adjustments where carefully tailored with performances and training time in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \n",
    "\n",
    "Here the training procedure. First of all I defined some callbacks (using the utility function) to be used in the fitting procedure (most importantly EarlyStopping for avoid overfitting and stopping the training) and fitted the model. \n",
    "\n",
    "Notice that I exploited the fact that the datasets created with `tf.data` are threadable and thus I did set: \n",
    "* `max_queue_size`, which is the number of batches to be kept in memory according to the values defined above (the number was tailored with lots of trial and error/crashes); \n",
    "* `workers` the number of threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weig = vqa.get_class_weights()\n",
    "call = vqa.callb()\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=50,\n",
    "    steps_per_epoch=e_steps, \n",
    "    validation_data=valid_dataset,\n",
    "    validation_steps=v_steps,\n",
    "    use_multiprocessing=True, \n",
    "    max_queue_size=max_que,\n",
    "    callbacks = call,\n",
    "    workers=8, \n",
    "    class_weight=weig,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the results\n",
    "\n",
    "Results are generated using the `model.predict()` method on the test dataset and applying an `argmax` operation in the output vector of predictions in order to select the most probable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vqa.make_prediction(model,test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
